---
layout: post
title: "第一讲：关于信息的基本概念"
date:   2023-05-31
tags: [信息论与编码讲义]
comments: true
author: 天马行空
---

> 并不是每一件事都重要，也并不是每一件重要的事我们都注意到了。——爱因斯坦

## 什么是信源？

一个随机变量就是一个信源，比如我们做的随机试验是投硬币，用随机变量 $\xi$来刻画这个随机现象，硬币为正面是时，$\xi$取 $1$，硬币为反面时，$\xi$取 $0$，从而 $\xi$就是一个信源，可能的取值为 $\{0,1\}$。



信源是一个黑盒，可以发出一些信号，我们将信源抽象为一个随机变量，具体地来说，在任何一个具体的时刻 $t$ ，信源对应的输出都是一个随机变量 $X_t$,我们称为信号。

根据 $t$的连续我们将信源可以分为

* 连续信源，当 $t$连续地取值时，如 $t\in [0,+\infty]$，显然，连续信源的数学模型是一个随机过程。
* 离散信源, 当 $t$离散地取值时，如 $t\in \mathbb N=\{0,1,2,\cdots\}$，离散信源的数学模型是一个随机序列。

根据信号 $X_t$的类型，我们可以将信源分为

* 模拟信源，当 $X_t$为连续型随机变量
* 数字信源，当 $X_t$为离散型随机变量

## 什么是一个随机事件的自信息量？

### 自信息量的定义

一个事件 $A$ 的自信息量定义为，$A$发生的概率的倒数的负对数，即
$$
I(A)=\log\dfrac1{P(A)}=-\log P(A)
$$

### 自信息量的性质

注意到，显然，一个随机事件的自信息量具有如下性质

* 非负
* 单调
* 可加性

对于非负性，这是显然的，因为对于某个随机事件 $A$ 而言，其概率 $P(A)\in [0,1]$，因此，其自信息量取值于 $I(A)\in [0,+\infty]$(在扩充实数域中，$+\infty$ 可以被取到)。但实际上，有一个问题是，对于几何概型而言，任何一个点对应的概率值都是 $0$，从而任何一个点对应的自信息量都是 $+\infty$，这一点又该如何理解呢？

对于单调性，我们知道的是，一个随机事件发生的概率越小，其发生给我们带来的自信息量就越大。

对于可加性，这当然，也是我们所期望的，两个相互独立事件 $A,B$各自的发生都不会对另一个随机事件的发生提供任何启示(指另一个随机事件的概率的变化)，因此，我们当然希望的是，这两件事同时发生的自信息量应当等于这两个随机事件的自信息量之和，即
$$
I(AB)=I(A)+I(B)
\label{2}
$$
显然，根据相互独立的两个随机事件的关系
$$
P(AB)=P(A)P(B)
$$
我们容易得到$\eqref{2}$中性质。

但是，对于一般的情况而言，一个朴素的认知是，当两个随机事件不是相互独立的情况下，一个随机事件的发生，多少会对另一个随机事件有所启示，在这种情况下，显然两个不相互独立的随机事件 $A,B$其同时发生的信息量就要小于这两个随机事件的自信息量之和，结合 $\eqref{2}$,我们容易有对于任意的随机事件 $A,B$，其对应的自信息量应当满足如下关系
$$
I(AB)\leq I(A)+I(B)
\label{4}
$$

### 自信息量的单位

注意到，不同的底数会对应不同的自信息量，因为自信息量的单位取决于对数的底数，之后，我们会更进一步地解释为什么要对单位进行区分。

一般而言，有两种单位

* 以 $2$为底，对应的单位为 $\text {bit}$，称为“比特”
* 以 $\text e$为底，对应的单位为 $\text {nat}$，称为“奈特”

下面，我们来解释这个单位的实际含义是什么，设想，我们想要用计算机储存一个随机试验的结果，假设一个随机试验总共有 $2^m$个等可能的结果，那么我们就可以用一个 $m$位的二进制数来储存这个随机试验的某个结果，一方面，这种储存方式是够用的(我们可以唯一地表示所有的基本事件)，另一方面，这些存储资源也刚好被完全用完。同时，我们计算一个这个随机事件的信息量，容易发现，刚好就是 $m\,\text {bit}$，更进一步的探索，我们在编码部分进行。

* $m$个比特组的信息量为 $m\,\text {bit}$
* 信息量为 $m\,\text {bit}$的随机事件需要用 $m$个比特组来存储

## 什么是互信息？

### 互信息量的定义

在前面的讨论中，注意到，两个不相互独立的随机事件 $A,B$之间是互相启发的，即一个随机事件的发生会对另一个随机事件的发生产生影响，同时注意到，根据 $\eqref{4}$，我们有任何两个随机事件同时发生的自信息量是不大于两个随机事件的自信息量之和的，即两个事件同时发生的情况下，事实上有一部分的重复的信息，我们自然地想到用如下的方式来度量这种重复的信息或者说事件 $A$与事件 $B$相互启发的信息，即
$$
I(A;B)=I(AB)-I(A)-I(B)
$$
如果我们考虑将其写成概率的形式，那么就有
$$
\begin{aligned}
I(A;B)
&=I(A)+I(B)-I(AB)\\
&=-\log P(A)-\log P(B)+\log P(AB)\\
&=\log \frac{P(AB)}{P(B)}-\log P(A)\\
&=\log P(A|B)-\log P(A)\\
&=\log \frac{P(A|B)}{P(A)} \\
\end{aligned}
$$
即两个随机事件的互信息量等于后验概率与先验之比的对数。

### 互信息量的性质

通过上述的推导，容易看出，互信息量具有如下性质

* 非负性
* 对称性

对于非负性，我们解释是一个随机事件的发生多少对另一个随机事件有一点启发。

对于对称性，实际含义是两个随机事件相互启发的信息量是一致的。

### 两种极端情况下的互信息

* 两个随机事件相互独立
* 一个事件的发生完全决定了另一个事件的发生

### 对互信息量的理解

从定义我们可以看出，互信息有两种含义

* 两个随机事件之间“重叠”的信息，或者说两个随机事件之间冗余的信息
* 一个随机事件的发生对另一个随机事件启发的信息

## 什么是条件自信息？

### 条件自信息的定义

两个不相互独立的事件 $A,B$，当事件 $A$已经发生条件下，会消除一部分 $B$的不确定性，从而使得 $B$的自信息量减少，此时，我们就把 $A$发生之后，$B$还剩下的信息量称为在 $A$发生的条件下，$B$的自信息量，即
$$
\begin{aligned}
I(B|A)&=I(B)-I(AB)\\
&=-\log(P(B|A))\\
&=\log \frac{1}{P(B|A)}
\end{aligned}
\label{7}
$$
从上述的变形中可以看到，这种定义方式是合理的。

### 条件自信息的性质

从上述的定义中，事实上，我们完全可以将条件自信息当做一个随机事件来处理，因为从形式上，条件自信息与自信息的是完全相同的，因此自信息具有的性质，条件自信息同样具有。特别地，条件自信息还具有一些特殊的性质，如 $\eqref{7}$的第一行。

