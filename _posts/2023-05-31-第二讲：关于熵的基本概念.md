---
layout: post
title: "第二讲：关于熵的基本概念"
date:   2023-05-31
tags: [信息论与编码讲义]
comments: true
author: 天马行空
---

在上一讲的讨论中，我们的主要研究对象是

* 一个事件的信息量 $I(A)$
* 两个事件的互信息量 $I(A;B)$
* 一个事件的条件信息量 $I(A\|B)$
* 两个事件的交事件的信息量 $I(AB)$

我们有关系

$$
I(AB)+I(A;B)=I(A)+I(B)
\\
I(AB)=I(A)+I(B|A)=I(B)+I(A|B)
$$

接下来，我们试着将一个随机变量作为一个整体进行研究，并将上述概念进一步推广为“熵”

* 信息熵 $H(X)$
* 平均互信息量 $I(X;Y)$
* 条件熵 $H(X\|Y)$
* 联合熵 $H(XY)$

同样的，我们也有关系

$$
H(XY)+I(X;Y)=H(X)+H(Y)
\\
H(XY)=H(X)+H(Y|X)=H(Y)+H(X|Y)
$$

## 什么是信息熵？

### 熵的定义

注意到，我们上述的讨论中，仅仅只涉及到了对于随机事件的信息量的刻画，而事实上，很多时候，我们会将一个随机变量作为一个整体考察其对应的信息量，因此，我们不妨将数学期望与自信息量结合起来考虑一个随机变量所对应的平均信息量。

设一个随机变量 $\xi$，即定义随机变量 $\xi$的平均信息量，即熵为

$$
\begin{aligned}
H(\xi)&=E(I(\xi))\\
&=\int_{-\infty}^{+\infty}I(x)\,\text d F_{\xi}(x)

\end{aligned}
$$

当 $\xi$为离散型随机变量，取值于 $\{x_1,x_2,\cdots\}$时，我们有

$$
\begin{aligned}
H(\xi)&=\sum_{k=1}^{\infty}I(x_k)p(x_k)\\
&=-\sum_{k=1}^{\infty}p(x_k)\log{p(x_k)}
\end{aligned}
$$

那么这个定义方式对于连续型随机变量，乃至于对于任意的随机变量是否是有意义的，或者说合理的呢？这个我们之后再进一步讨论。但至少，基于上述的定义方式，我们定义出了离散型随机变量的熵。

### 熵的性质

* 非负性

### 最大熵原理

我们呢考虑一个简单的问题，对于一个取值有限集合 $\{x_1,x_2,\cdots,x_n\}$ 的随机变量 $\xi$，其概率应当如何分布，$H(\xi)$才最大？

即考虑如下问题，设有 $n$个正实数 $p_1,p_2,\cdots,p_n$，满足

$$
\sum_{k=1}^np_k=1
$$

考察多元函数

$$
f(p_1,p_2,\cdots,p_n)=-\sum_{k=1}^np_k\ln{p_k}
$$

何时取得极值，极值几何？

用拉格朗日乘数法，构造拉式量

$$
L(p_1,p_2,\cdots,p_n;\lambda)=-\sum_{k=1}^np_k\ln{p_k}+\lambda\left(\sum_{k=1}^np_k-1\right)
$$

从而我们有如下取极值的条件

$$
\begin{cases}
\dfrac{\partial L}{\partial p_i}=\lambda-\ln p_i-1=0,\,i\in\{1,2,\cdots,n\}
\\
\displaystyle \sum_{k=1}^np_k=1
\end{cases}
$$

从而可以解得

$$
\begin{cases}
\lambda=1-\ln n
\\
p_k=\dfrac1n,\,k\in\{1,2,\cdots,n\}
\end{cases}
$$

即当且仅当概率是均匀分布的情况下，熵是最大，即有

$$
H(\xi)\leq \log n
$$

## 什么是平均互信息？

对于两个随机变量 $\xi,\eta$,我们可以定义他们的平均互信息为

$$
I(\xi;\eta)=E(I(\xi;\eta))
$$

特别地，对于离散型随机变量 $X$取值于 $\{x_1,x_2,\cdots,x_m\}$，随机变量 $Y$取值于 $\{y_1,y_2,\cdots,y_n\}$,则有 $X,Y$的平均互信息为

$$
\begin{aligned}
I(X;Y)&=\sum_{i=1}^m\sum_{j=1}^np(x_i,y_j)I(x_i,y_j)\\
&=-\sum_{i=1}^m\sum_{j=1}^np(x_i,y_j)\log \dfrac{p(x_i|y_j)}{p(x_i)}\\
&=-\sum_{i=1}^m\sum_{j=1}^np(x_i,y_j)\log \dfrac{p(y_j|x_i)}{p(y_j)}
\end{aligned}
$$

### 平均互信息的性质

* 非负性
* 对称性

### 两种极端情况下的互信息

平均互信息可以作为一个指标来衡量连个变量之间的独立程度，如果两个随机变量是相互独立的，那么显然有 $I(X;Y)=0$

而当两个随机变量完全相同时，即考察 $I(X;X)$,则显然易得 $I(X;X)=H(X)$

## 什么是条件熵？

条件自信息的数学期望,对于离散型随机变量 $X$取值于 $\{x_1,x_2,\cdots,x_m\}$，随机变量 $Y$取值于 $\{y_1,y_2,\cdots,y_n\}$,则有 $X,Y$的条件熵为

$$
\begin{aligned}
H(Y|X)&=\sum_{i=1}^m\sum_{j=1}^np(x_i,y_j)I(y_j|x_i)\\
&=-\sum_{i=1}^m\sum_{j=1}^np(x_i,y_j)\log{p(y_j|x_i)}

\end{aligned}
$$

相应的有

$$
\begin{aligned}
H(X|Y)&=\sum_{i=1}^m\sum_{j=1}^np(x_i,y_j)I(x_i|y_j)\\
&=-\sum_{i=1}^m\sum_{j=1}^np(x_i,y_j)\log{p(x_i|y_j)}

\end{aligned}
$$

### 条件熵是否还是某个分布的熵

当 $Y$为某个确定的随机事件时，此时的条件熵是在 $Y$对应的随机事件发生后随机变量 $X$重新分布后的得到的新的条件随机变量的熵，但是对于一般的条件熵，而言，则并具备这样的性质。

## 什么是联合熵？

条件自信息的数学期望,对于离散型随机变量 $X$取值于 $\{x_1,x_2,\cdots,x_m\}$，随机变量 $Y$取值于 $\{y_1,y_2,\cdots,y_n\}$,则有 $X,Y$的联合熵为

$$
\begin{aligned}
H(XY)&=\sum_{i=1}^m\sum_{j=1}^np(x_i,y_j)I(x_iy_j)\\
&=\sum_{i=1}^m\sum_{j=1}^np(x_i,y_j)\log{\dfrac{1}{p(x_i,y_j)}}\\
&=-\sum_{i=1}^m\sum_{j=1}^np(x_i,y_j)\log{{p(x_i,y_j)}}
\end{aligned}
$$

## 几种熵之间的关系

